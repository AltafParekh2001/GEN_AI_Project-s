{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AltafParekh2001/GEN_AI_Project-s/blob/main/TextSummarizer_withGradio_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dtyoWpmxexp"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries: Gradio for building web interfaces and Transformers for NLP models.\n",
        "!pip install gradio\n",
        "!pip install transformers\n",
        "\n",
        "# Import PyTorch and Gradio library.\n",
        "import torch\n",
        "import gradio\n",
        "\n",
        "# Print the PyTorch version and check if CUDA (GPU support) is available.\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pipeline function from the transformers library.\n",
        "# This is a high-level helper for using pre-trained models.\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize a summarization pipeline using a pre-trained 'distilbart-cnn-12-6' model.\n",
        "# The torch_dtype is set to torch.bfloat16 for potentially better performance or memory usage.\n",
        "pipe = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\",torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "8en8dgMkyATy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a variable to store a specific model path within the Hugging Face cache directory.\n",
        "model_path = ('/root/.cache/huggingface/hub/models/sshleifer--distilbart-cnn-12-6')"
      ],
      "metadata": {
        "id": "Y6-Rca8H6Lvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa969af9"
      },
      "source": [
        "import os\n",
        "from transformers import TRANSFORMERS_CACHE\n",
        "\n",
        "# Print the Hugging Face cache directory path.\n",
        "print(f\"Hugging Face cache directory: {TRANSFORMERS_CACHE}\")\n",
        "\n",
        "# Define the name of the model being used.\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "# Construct a potential path to where the model files might be stored within the cache.\n",
        "# Hugging Face replaces '/' in model names with '--' in directory paths.\n",
        "model_path_guess = os.path.join(TRANSFORMERS_CACHE, 'models', model_name.replace('/', '--'))\n",
        "print(f\"Potential model path (you might need to explore this directory): {model_path_guess}\")\n",
        "\n",
        "# Example of how to list contents or clear the cache (commented out for safety).\n",
        "# You can list the contents of the cache directory to find the exact location\n",
        "# import shutil\n",
        "# shutil.rmtree(TRANSFORMERS_CACHE) # Use with caution, this will clear your cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to perform summarization using the pre-trained pipeline.\n",
        "def summary(input):\n",
        "  # Call the summarization pipeline with the input text.\n",
        "  output = pipe(input)\n",
        "  # Return the 'summary_text' from the pipeline's output.\n",
        "  return output[0]['summary_text']\n",
        "\n",
        "# Close any existing Gradio interfaces to prevent conflicts or port issues.\n",
        "gr.close_all()\n",
        "\n",
        "# Create a more structured Gradio interface for the summarization function.\n",
        "# It includes a Textbox for input (with a label and multiple lines) and a Textbox for output.\n",
        "# Also adds a title and description for the application.\n",
        "demo = gr.Interface(fn=summary,\n",
        "                    inputs=[gr.Textbox(label=\"Input text to summarize\",lines=6)],\n",
        "                    outputs=[gr.Textbox(label=\"Summarized text\",lines=4)],\n",
        "                    title=\"Project 1: Text Summarizer\",\n",
        "                    description=\"THIS APPLICATION WILL BE USED TO SUMMARIZE THE TEXT\")\n",
        "\n",
        "# Launch the Gradio interface, sharing it publicly.\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "afBkcBWW7mEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49aM1rma8pct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}